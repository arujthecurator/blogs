[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hello, my name is Aruj Nayak. I am a results-driven software developer with a strong educational background, holding a Master’s and a Bachelor’s degree in Computer Science & Applications. Proficient in languages like C++, Python, and Java, I bring a diverse skill set to software development. My experience involves leading teams, optimizing workflows, and achieving significant improvements in internal software systems. My commitment to continuous learning reflects in my versatile project portfolio. Eager to contribute my technical proficiency and problem-solving skills to impactful software development and machine learning projects."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning blogs for the course CS 5805",
    "section": "",
    "text": "Anomaly Detection Demonstrated in the Forest Cover Types Dataset\n\n\n\n\n\n\n\nAnomaly Detection\n\n\n\n\n\n\n\n\n\n\n\nDec 6, 2023\n\n\nAruj Nayak\n\n\n\n\n\n\n  \n\n\n\n\nClassification: Using Balanced Bagging Classifier\n\n\n\n\n\n\n\nClassification\n\n\n\n\n\n\n\n\n\n\n\nDec 6, 2023\n\n\nAruj Nayak\n\n\n\n\n\n\n  \n\n\n\n\nClustering: Agglomerative Clustering\n\n\n\n\n\n\n\nClustering\n\n\n\n\n\n\n\n\n\n\n\nDec 6, 2023\n\n\nAruj Nayak\n\n\n\n\n\n\n  \n\n\n\n\nLinear and Non-Linear Regression\n\n\n\n\n\n\n\nLinear & Non-Linear Regression\n\n\n\n\n\n\n\n\n\n\n\nDec 6, 2023\n\n\nAruj Nayak\n\n\n\n\n\n\n  \n\n\n\n\nProbability theory and random variables: Bayesian Networks\n\n\n\n\n\n\n\nProbability theory & Random Variables\n\n\n\n\n\n\n\n\n\n\n\nDec 6, 2023\n\n\nAruj Nayak\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Linear_and_Non_Linear_Regression/index.html",
    "href": "posts/Linear_and_Non_Linear_Regression/index.html",
    "title": "Linear and Non-Linear Regression",
    "section": "",
    "text": "Exploring Linear and Non-linear Regression in Machine Learning\n\n\nIntroduction:\nLinear and non-linear regression are essential techniques in the realm of machine learning and statistics, commonly utilized for predicting numerical values based on input features. In this blog, we delve into the core concepts of linear and non-linear regression and provide practical examples using two prominent algorithms: Support Vector Machine (SVM) for linear regression and Gradient Boosting Regressor for non-linear regression.\n\n\nLinear Regression:\nLinear regression is a straightforward and interpretable method used for modeling the relationship between a dependent variable and one or more independent variables. This method assumes a linear relationship, signifying that the change in the dependent variable is proportionate to the change in the independent variables. The equation for simple linear regression can be expressed as:\n[ y = mx + b ]\nHere: - ( y ): Dependent variable - ( x ): Independent variable - ( m ): Slope of the line - ( b ): Y-intercept\n\n\nImplementation:\nIn this blog, we implemented linear regression using the scikit-learn library with the Support Vector Machine (SVM) algorithm. In this context, SVM with a linear kernel functions as a linear regressor. We utilized the “diamonds” dataset from seaborn, considering attributes such as carat, depth, table, and dimensions (x, y, z) to predict diamond prices. The following code snippet demonstrates this-\n\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\nimport seaborn as sns\n\n# Load the diamonds dataset\ndiamonds = sns.load_dataset(\"diamonds\")\n\n# Select features and target variable\nX = diamonds[['carat', 'depth', 'table', 'x', 'y', 'z']]\ny = diamonds['price']\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create and train the support vector regressor with linear kernel\nlinear_svm_model = SVR(kernel='linear')\nlinear_svm_model.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred_linear_svm = linear_svm_model.predict(X_test)\n\n# Evaluate the model\nr2_linear_svm=r2_score(y_test,y_pred_linear_svm)\nprint(f\"r2 score (Linear SVM): {r2_linear_svm}\")\n\nr2 score (Linear SVM): 0.7627450131825717\n\n\nThe actual vs predicted labels for the test data predicted by this model can be visualized in order to understand how good our model is performing-\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Visualize predicted vs actual for linear regression\nplt.scatter(y_test, y_pred_linear_svm, alpha=0.5, label='Actual vs Predicted')\nplt.plot(np.unique(y_test), np.poly1d(np.polyfit(y_test, y_pred_linear_svm, 1))(np.unique(y_test)), color='red', label='Best Fit Line')\nplt.title(\"Linear Regression: Predicted vs Actual\")\nplt.xlabel(\"Actual Prices\")\nplt.ylabel(\"Predicted Prices\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\nNon-Linear Regression-\nWhile linear regression is effective for linear relationships, real-world scenarios often involve non-linear patterns. Non-linear regression extends the concept to model relationships where the change in the dependent variable is not proportional to the change in the independent variables.\n\n\nImplementation-\nFor non-linear regression, we employed the Gradient Boosting Regressor, an ensemble method that combines weak learners to create a robust predictive model. Using the same “diamonds” dataset, this algorithm can capture intricate patterns in the data. Following code show the implementation of this model-\n\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.metrics import r2_score\n\n# Create and train the gradient boosting regressor\ngradient_boosting_model = GradientBoostingRegressor(n_estimators=100, random_state=42)\ngradient_boosting_model.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred_gradient_boosting = gradient_boosting_model.predict(X_test)\n\n# Evaluate the model\nr2_gradient_boosting=r2_score(y_test,y_pred_gradient_boosting)\nprint(f\"r2 score (Gradient Boosting): {r2_gradient_boosting}\")\n\nr2 score (Gradient Boosting): 0.8863745168393169\n\n\nAnd just like linear regression, we can visualize the predictions made by this model-\n\n# Visualize predicted vs actual for non-linear regression\nplt.scatter(y_test, y_pred_gradient_boosting, alpha=0.5, label='Actual vs Predicted')\nplt.plot(np.unique(y_test), np.poly1d(np.polyfit(y_test, y_pred_gradient_boosting, 1))(np.unique(y_test)), color='red', label='Best Fit Line')\nplt.title(\"Non-linear Regression: Predicted vs Actual\")\nplt.xlabel(\"Actual Prices\")\nplt.ylabel(\"Predicted Prices\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\nConclusion:\nThus, linear and non-linear regression are versatile tools for modeling relationships in data. Linear regression provides simplicity and interpretability, making it suitable for scenarios with linear dependencies. Conversely, non-linear regression techniques, like the Gradient Boosting Regressor, offer the flexibility to capture complex patterns in the data, making them valuable in various applications.\nWhen working with real-world datasets, understanding the nature of relationships within the data is crucial for choosing the appropriate regression model. Linear and non-linear regression methods complement each other, enabling data scientists and analysts to address a diverse array of predictive modeling tasks with precision and accuracy.\n\n\nSource:\nThe image is taken from- https://www.skillshare.com/en/classes/Data-Analysis-What-is-Non-Linear-Regression/694158109"
  },
  {
    "objectID": "posts/Classification/index.html",
    "href": "posts/Classification/index.html",
    "title": "Classification: Using Balanced Bagging Classifier",
    "section": "",
    "text": "Classification: Using Balanced Bagging Classifier\n\n\nIntroduction:\nIn the realm of machine learning, dealing with class imbalance is a common challenge that can significantly impact model performance. In this blog, we delve into the application of balanced bagging classifiers using the Pima Indians Diabetes dataset. Additionally, we harness the power of Principal Component Analysis (PCA) to gain visual insights into decision boundaries within a reduced-dimensional space.\n\n\nDefining the Decision Boundary Method:\nWe begin by importing essential libraries and defining a function to plot the decision boundary so as to properly visualize how target variable is classified.\n\n# Import the libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom imblearn.ensemble import BalancedBaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Define a function to plot decision boundaries\ndef plot_decision_boundary(model, X, y, title):\n    h = .02  # step size in the mesh\n    x_min, x_max = X.iloc[:, 0].min() - 1, X.iloc[:, 0].max() + 1\n    y_min, y_max = X.iloc[:, 1].min() - 1, X.iloc[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    plt.contourf(xx, yy, Z, alpha=0.3)\n    plt.scatter(X.iloc[:, 0], X.iloc[:, 1], c=y, edgecolors='k', marker='o', s=80, linewidth=0.8)\n    plt.title(title)\n    plt.xlabel(X.columns[0])\n    plt.ylabel(X.columns[1])\n    plt.show()\n\n\n\nClass Distribution Before Balancing:\nFor this blog, we will use the Pima Indians Diabetes (Pima) dataset which is freely available online. This dataset contains crucial health parameters, like BloodPressure, SkinThickness, etc. Each record in this dataset describes the medical details of a female, and the prediction is the onset of diabetes within the next five years. A visual inspection of the class distribution before any balancing techniques reveals the initial state of the data.\n\n# Load the Pima dataset\nurl = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv'\ncolumn_names = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']\ndata = pd.read_csv(url, names=column_names)\n\n# Plot class distribution before balancing\nplt.figure(figsize=(8, 5))\nsns.countplot(x='Outcome', data=data)\nplt.title('Class Distribution Before Balancing')\nplt.xlabel('Outcome (0: No Diabetes, 1: Diabetes)')\nplt.ylabel('Count')\nplt.show()\n\n\n\n\n\n\nUsing Random Under-Sampling:\nTo address class imbalance, we employ the Random Under-Sampling technique, strategically removing instances from the majority class. The resulting balanced dataset is visualized for a clearer understanding.\n\n# Apply balanced sampling techniques using RandomUnderSampler\nfrom imblearn.under_sampling import RandomUnderSampler\n\n# Split the data into features (X) and target variable (y)\nX = data.drop('Outcome', axis=1)\ny = data['Outcome']\n\n# Use RandomUnderSampler for under-sampling\nrus = RandomUnderSampler(random_state=42)\nX_resampled, y_resampled = rus.fit_resample(X, y)\n\n# Create a DataFrame with the balanced data\nbalanced_data = pd.DataFrame(X_resampled, columns=column_names[:-1])\nbalanced_data['Outcome'] = y_resampled\n\n# Visualize the balanced class distribution\nplt.figure(figsize=(8, 5))\nsns.countplot(x='Outcome', data=balanced_data)\nplt.title('Class Distribution After Balancing')\nplt.xlabel('Outcome (0: No Diabetes, 1: Diabetes)')\nplt.ylabel('Count')\nplt.show()\n\n\n\n\n\n\nModel Training with PCA:\nThis balanced data is now utilized for model training. We split the dataset, standardize features, and integrate PCA to reduce dimensionality, capturing essential patterns for our models.\n\n#Assign the features\nfeature_names = column_names[:-1]\n\n# Split the data into features (X) and target variable (y)\nX = balanced_data.drop('Outcome', axis=1)\ny = balanced_data['Outcome']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Standardize the features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Apply PCA to reduce dimensionality\npca = PCA(n_components=2)\nX_train_pca = pca.fit_transform(X_train_scaled)\nX_test_pca = pca.transform(X_test_scaled)\n\n\n\nDecision Boundaries of the Base Model and Balanced Bagging Classifier:\nFinally, a base model (Decision Tree) and an ensemble model (the Balanced Bagging Classifier) are trained. Their decision boundaries are then plotted so as to visualize the difference in classification of the target variable after balancing the classes. We have plotted the decision boundaries of both the base model (trained on the imbalanced dataset) and the Balanced Bagging Classifier within the reduced-dimensional space obtained through PCA.\n\n# Define and train a base model (e.g., Decision Tree)\nbase_model = DecisionTreeClassifier(random_state=42)\nbase_model.fit(X_train_pca, y_train)\n\n# Initialize and train a Balanced Bagging Classifier\nbalanced_bagging_model = BalancedBaggingClassifier(base_model, random_state=42)\nbalanced_bagging_model.fit(X_train_pca, y_train)\n\n# Visualize decision boundaries of the base model\nplt.figure(figsize=(10, 6))\nplot_decision_boundary(base_model, pd.DataFrame(X_train_pca, columns=['Principal Component 1', 'Principal Component 2']), y_train, 'Decision Boundary of base model (Imbalanced Data)')\n\n# Visualize decision boundaries of the balanced bagging classifier\nplt.figure(figsize=(10, 6))\nplot_decision_boundary(balanced_bagging_model, pd.DataFrame(X_train_pca, columns=['Principal Component 1', 'Principal Component 2']), y_train, 'Decision Boundaries of Balanced Bagging Classifier')\n\n\n\n\n\n\n\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n\n# Function to evaluate a model and print metrics\ndef evaluate_model(model, X, y):\n    y_pred = model.predict(X)\n    accuracy = accuracy_score(y, y_pred)\n    precision = precision_score(y, y_pred)\n    recall = recall_score(y, y_pred)\n    f1 = f1_score(y, y_pred)\n    \n    print(f\"Accuracy: {accuracy:.4f}\")\n    print(f\"Precision: {precision:.4f}\")\n    print(f\"Recall: {recall:.4f}\")\n    print(f\"F1 Score: {f1:.4f}\")\n    \n    # Confusion matrix\n    cm = confusion_matrix(y, y_pred)\n    print(\"\\nConfusion Matrix:\")\n    print(cm)\n\n# Evaluate the base model\nprint(\"Base Model Evaluation:\")\nevaluate_model(base_model, X_test_pca, y_test)\n\n# Evaluate the improved Balanced Bagging Classifier\nprint(\"\\nImproved Balanced Bagging Classifier Evaluation:\")\nevaluate_model(balanced_bagging_model, X_test_pca, y_test)\n\nBase Model Evaluation:\nAccuracy: 0.5926\nPrecision: 0.5424\nRecall: 0.6531\nF1 Score: 0.5926\n\nConfusion Matrix:\n[[32 27]\n [17 32]]\n\nImproved Balanced Bagging Classifier Evaluation:\nAccuracy: 0.6667\nPrecision: 0.6226\nRecall: 0.6735\nF1 Score: 0.6471\n\nConfusion Matrix:\n[[39 20]\n [16 33]]\n\n\n\n\nConclusion:\nIn this blog, we’ve navigated through the intricacies of handling class imbalance using balanced bagging classifiers and illuminating decision boundaries through PCA. This approach not only improves model robustness but also enhances interpretability, crucial for making informed decisions.\n\n\nSource:\nThe dataset is prepared by Jason Brownlee and can be found here- Pima Indians Diabetes dataset.\nThe Image is taken from- https://datagalore.substack.com/p/class-imbalance-problem"
  },
  {
    "objectID": "posts/Anomaly_Detection_Forest_Cover/index.html",
    "href": "posts/Anomaly_Detection_Forest_Cover/index.html",
    "title": "Anomaly Detection Demonstrated in the Forest Cover Types Dataset",
    "section": "",
    "text": "Anomaly Detection Demonstrated in the Forest Cover Types Dataset\n\n\nIntroduction:\nUnderstanding Anomaly Detection is a vital component as it involves identifying patterns or instances that stand out from the majority of data points. Anomalies can signify potential fraud, errors, or abnormal behavior, making their detection critical for decision-making and data quality. In the vast landscape of data, anomalies or outliers can provide crucial insights or indicate potential issues. Thus, this field of Data Analysis specifically focuses on identifying these instances that deviate significantly from the norm within a dataset. This blog post introduces two popular anomaly detection methods: Local Outlier Factor (LOF) and Isolation Forest.\n\n\nLocal Outlier Factor Anomaly Detection Method:\nLocal Outlier Factor is a robust anomaly detection algorithm, evaluating the local density deviation of individual data points. This method operates by contrasting the density of a specific data point with that of its neighboring points. Instances demonstrating substantially lower density in comparison to their neighbors are assigned higher anomaly scores, signifying their likelihood of being outliers. LOF proves very effective in situations where anomalies are not necessarily far removed from the majority but exhibit a distinctive characteristic of lower density.\n\n\nIsolation Forest Anomaly Detection Method:\nIsolation Forest takes a different approach by isolating anomalies rather than profiling normal instances. It constructs a tree-based structure, isolating anomalies in fewer steps due to their uniqueness. Anomalies are isolated more quickly in the tree, making the isolation path shorter. Isolation Forest is efficient, scalable, and often performs well on high-dimensional data.\n\n\nDataset:\nThe Forest Cover Types dataset is commonly used for classification tasks. It comprises of cartographic variables, including topography, soil types, and wilderness areas, to predict forest cover types. In our anomaly detection example, we consider a specific forest cover type as anomalies, aiming to detect unusual instances in the dataset.\n\nimport pandas as pd\nfrom sklearn.datasets import fetch_covtype\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.neighbors import LocalOutlierFactor\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\n# Loading the Forest Cover Types dataset\ncover_types = fetch_covtype()\ndata = pd.DataFrame(data=cover_types.data, columns=[f'feature_{i}' for i in range(cover_types.data.shape[1])])\ndata['target'] = cover_types.target\n\nThe above section involves importing the required python libraries and loading the Forest Cover Types dataset using fetch_covtype. The data is then converted into a Pandas DataFrame for ease of manipulation.\nThe following code snippet demonstrates a specific forest cover type (type 1) as anomalies, creating separate datasets for normal and anomaly instances. A combined dataset is then formed for visualization, wich comprises of a subset of normal data and all anomaly data.\n\n# Simplifying by considering one forest cover type (e.g., type 1) as anomalies\n# This can be adjusted based on specific use cases\nanomaly_label = 1\nnormal_data = data[data['target'] != anomaly_label]\nanomaly_data = data[data['target'] == anomaly_label]\n\n# Combining a subset of normal data with anomaly data for visualization\nsubset_normal_data = normal_data.sample(n=1000, random_state=42)\ncombined_data = pd.concat([subset_normal_data, anomaly_data])\n\nNext, Principal Component Analysis (PCA) is applied to reduce dimensionality of this dataset by reducing to two principal components, capturing the most significant variations.\n\n# Employing PCA for dimensionality reduction\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(combined_data.drop('target', axis=1))\n\nFinally, the Isolation Forest is employed to visually represent the anomalies in a scatter plot using the reduced PCA space.\n\n# Applying Isolation Forest\nmodel_iforest = IsolationForest(contamination=0.05)\npredictions_iforest = model_iforest.fit_predict(combined_data.drop('target', axis=1))\n# Visualizing Isolation Forest results\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.scatter(X_pca[:, 0], X_pca[:, 1], c=predictions_iforest, cmap='viridis')\nplt.title('Isolation Forest Anomaly Detection')\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\n\nText(0, 0.5, 'Principal Component 2')\n\n\n\n\n\nAdditionally, a subplot is also created to compare the results obtained using Local Outlier Factor Method to show the difference between these two anomaly detection methods.\n\n# Implementing Local Outlier Factor (LOF)\nmodel_lof = LocalOutlierFactor(n_neighbors=20, contamination=0.05)\npredictions_lof = model_lof.fit_predict(combined_data.drop('target', axis=1))\n\n# Visualizing Local Outlier Factor results\nplt.subplot(1, 2, 2)\nplt.scatter(X_pca[:, 0], X_pca[:, 1], c=predictions_lof, cmap='viridis')\nplt.title('Local Outlier Factor Anomaly Detection')\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.show()\n\n\n\n\nThus, anomaly detection proves indispensable in soil microbial analysis and forest coverage assessment, forging a critical link between its importance and the preservation of environmental ecosystems. By pinpointing irregularities in microbial dynamics or unexpected changes in forest vegetation through anomaly detection, we enhance our ability to detect potential threats early on.These methods demonstrate the broader importance of anomaly detection tehcniques in providing actionable insights for environmental conservation and ensuring the long-term health and resilience of ecosystems.\n\n\nSource:\nImage is taken from- https://anomify.ai/blogs/what-is-anomaly-detection"
  },
  {
    "objectID": "posts/Clustering/index.html",
    "href": "posts/Clustering/index.html",
    "title": "Clustering: Agglomerative Clustering",
    "section": "",
    "text": "Agglomerative Clustering Demonstrated Using Seeds Dataset\n\n\nIntroduction:\nIn the realm of unsupervised machine learning, agglomerative clustering stands out as a powerful tool for unraveling hidden structures within datasets. In this blog post, we will see how agglomerative clustering works while exploring its application on the Seeds dataset. The Seeds dataset which is a collection of measurements related to various seed properties, is a very simple data set to understand this clustering technique.\n\n\nSeeds Dataset:\nThe Seeds dataset, sourced from the UCI Machine Learning Repository, encapsulates the characteristics of three different varieties of wheat seeds: Kama, Rosa, and Canadian. The dataset comprises of seven distinct features (‘V1’ to ‘V7’),which are based on seed atrributes like area, perimeter, compactness, etc. measured for each seed.\n\n\nExploration and Preprocessing:\nBefore delving into clustering, it’s crucial to acquaint ourselves with the features at hand. Visualization aids in this process, and histograms or scatter plots can reveal the distribution and relationships within the dataset. Moreover, standardizing or scaling features may be necessary to ensure each contributes equally to the clustering process.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import datasets\n\n# Load the Seeds dataset\nseeds = datasets.fetch_openml(name='seeds', version=1)\nX = pd.DataFrame(data=seeds.data, columns=seeds.feature_names)\n\n# Visualize features (example: scatter plot of v1 and v2)\nplt.scatter(X['V1'], X['V2'])\nplt.title('Scatter Plot of Feature v1 vs. Feature v2')\nplt.xlabel('Feature v1')\nplt.ylabel('Feature v2')\nplt.show()\n\n# Standardize features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\nC:\\Users\\USER\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\datasets\\_openml.py:1022: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n  warn(\n\n\n\n\n\n\n\nUnderstanding Agglomerative Clustering:\nAgglomerative clustering builds clusters hierarchically by successively merging or agglomerating individual data points or clusters. The process continues until all data points belong to a single cluster or a specified number of clusters is reached. The choice of linkage method, such as ‘ward,’ ‘complete,’ or ‘average,’ influences how the distance between clusters is calculated during the agglomeration process.\n\n\nClustering Seeds with Ward Linkage:\nThe Ward linkage minimizes the variance within clusters, aiming to create compact and spherical clusters. This method is sensitive to the distribution of data points and often results in well-defined clusters.\n\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.metrics import silhouette_score\n\n# Perform agglomerative clustering with Ward linkage\nn_clusters = 3\nagg_clustering_ward = AgglomerativeClustering(n_clusters=n_clusters, linkage='ward')\nlabels_ward = agg_clustering_ward.fit_predict(X_scaled)\n\n# Silhouette score for Ward linkage\nsilhouette_avg_ward = silhouette_score(X_scaled, labels_ward)\nprint(f'Ward Linkage - Silhouette Score: {silhouette_avg_ward}')\n\nWard Linkage - Silhouette Score: 0.3926339709101015\n\n\n\n\nClustering Seeds with Complete Linkage:\nIn contrast, the Complete linkage measures the maximum distance between clusters. It tends to form clusters with similar shapes and sizes, making it less sensitive to outliers compared to Ward linkage.\n\n# Perform agglomerative clustering with Complete linkage\nagg_clustering_complete = AgglomerativeClustering(n_clusters=n_clusters, linkage='complete')\nlabels_complete = agg_clustering_complete.fit_predict(X_scaled)\n\n# Silhouette score for Complete linkage\nsilhouette_avg_complete = silhouette_score(X_scaled, labels_complete)\nprint(f'Complete Linkage - Silhouette Score: {silhouette_avg_complete}')\n\nComplete Linkage - Silhouette Score: 0.35019845816108097\n\n\n\n\nVisual Comparison\nLet’s visualize the clusters formed by both Ward and Complete linkage methods to observe the distinctions. The scatter plots showcase how each method influences the grouping of seeds based on their features.\n\n# Visualize clusters for Ward Linkage\nplt.scatter(X['V1'], X['V2'], c=labels_ward, cmap='viridis')\nplt.title('Agglomerative Clustering (Ward Linkage) of Seeds Dataset')\nplt.xlabel('Feature v1')\nplt.ylabel('Feature v2')\nplt.show()\n\n# Visualize clusters for Complete Linkage\nplt.scatter(X['V1'], X['V2'], c=labels_complete, cmap='viridis')\nplt.title('Agglomerative Clustering (Complete Linkage) of Seeds Dataset')\nplt.xlabel('Feature v1')\nplt.ylabel('Feature v2')\nplt.show()\n\n\n\n\n\n\n\n\n\nCluster Profiles\nAs clusters materialize, it’s time to interpret their meaning. Examining the average values of each feature within a cluster unveils distinct profiles. Are certain clusters dominated by larger seeds, or do others exhibit unique combinations of area and compactness?\n\n\nValidation and Interpretation Using Silhouette Score:\nThe silhouette score is a metric used to calculate the goodness of a clustering technique. It measures how similar an object is to its own cluster (cohesion) compared to other clusters (separation). The silhouette score ranges from -1 to 1, where a higher score indicates better-defined clusters. A score around 0 indicates overlapping clusters. Validation metrics, such as silhouette scores, offer quantitative measures of clustering quality. Moreover, comparing clustering results across different linkage methods can shed light on the sensitivity of the algorithm to choices made during clustering.\n\n\nConclusion\nIn conclusion, agglomerative clustering’s hierarchical nature provides a nuanced understanding of seed properties, enabling us to discern patterns that might otherwise remain concealed. This blog tried to illustrate the basic usage of agglomerative clustering to understand how unsupervised machine learning algorithms can be used.\n\n\nSource:\nImage is taken from- https://online.keele.ac.uk/clustering-and-cluster-analysis-an-explainer/"
  },
  {
    "objectID": "posts/Probability_theory_and_random_variables/index.html",
    "href": "posts/Probability_theory_and_random_variables/index.html",
    "title": "Probability theory and random variables: Bayesian Networks",
    "section": "",
    "text": "Probability theory and random variables: Bayesian Networks\n\n\nIntroduction:\nBayesian Networks, often referred to as Bayesian Belief Networks, are powerful probabilistic graphical models used to analyze and represent complex systems. Rooted in probability theory, these models provide a structured approach to capture intricate relationships among variables.\n\n\nNodes, Variables, and Conditional Probability Tables (CPTs):\nIn Bayesian Networks, nodes represent random variables, and edges indicate probabilistic dependencies, forming a Directed Acyclic Graph (DAG). Each node is associated with a Conditional Probability Table (CPT), detailing the conditional probabilities of a variable given its parent variables. These tables encode the probabilistic relationships inherent in the network.\n\n\nDirected Acyclic Graph (DAG):\nThe network structure follows a Directed Acyclic Graph, ensuring no cycles. This acyclic structure facilitates efficient inference and minimizes logical inconsistencies.\n\n\nInference:\nBayesian Networks enable probabilistic inference, computing probabilities for unobserved variables given observed evidence. This functionality proves valuable for decision-making and handling uncertainty.\n\n\nBayesian Network for Car Evaluations:\nConsider assessing cars dataset based on buying price, maintenance cost, number of doors, persons capacity, luggage boot size, and safety rating. A Bayesian Network models the probabilistic dependencies between these variables as is demonstrated by the following code snippet-\n\nimport daft\nfrom pgmpy.models import BayesianNetwork  # Update the import statement\nfrom pgmpy.inference import VariableElimination\nimport pandas as pd\n\n# Load the Car Evaluation dataset from UCI ML Repository\nurl = \"https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data\"\ncolumn_names = [\"buying\", \"maint\", \"doors\", \"persons\", \"lug_boot\", \"safety\", \"class\"]\ndata = pd.read_csv(url, names=column_names)\n\n# Define the structure of the Bayesian Network\nmodel = BayesianNetwork([('buying', 'class'), ('maint', 'class'), ('doors', 'class'),\n                          ('persons', 'class'), ('lug_boot', 'class'), ('safety', 'class')])\n\n# Fit the Bayesian Network using the dataset\nmodel.fit(data)\n\n# Render the Bayesian Network using daft\npgm = daft.PGM()\n\npgm.add_node(\"buying\", r\"buying\", 1, 1)\npgm.add_node(\"maint\", r\"maint\", 2, 1)\npgm.add_node(\"doors\", r\"doors\", 3, 1)\npgm.add_node(\"persons\", r\"persons\", 4, 1)\npgm.add_node(\"lug_boot\", r\"lug_boot\", 5, 1)\npgm.add_node(\"safety\", r\"safety\", 6, 1)\npgm.add_node(\"class\", r\"class\", 3.5, 2)\n\npgm.add_edge(\"buying\", \"class\")\npgm.add_edge(\"maint\", \"class\")\npgm.add_edge(\"doors\", \"class\")\npgm.add_edge(\"persons\", \"class\")\npgm.add_edge(\"lug_boot\", \"class\")\npgm.add_edge(\"safety\", \"class\")\n\n# Render the Bayesian Network\npgm.render()\npgm.show()\n\n# Perform inference to predict the class given evidence\ninference = VariableElimination(model)\nresult = inference.query(variables=['class'], evidence={'buying': 'vhigh', 'maint': 'vhigh'})\n\nprint(result)\n\n\n\n\n+--------------+--------------+\n| class        |   phi(class) |\n+==============+==============+\n| class(acc)   |       0.0000 |\n+--------------+--------------+\n| class(good)  |       0.0000 |\n+--------------+--------------+\n| class(unacc) |       1.0000 |\n+--------------+--------------+\n| class(vgood) |       0.0000 |\n+--------------+--------------+\n\n\nIn the above example, the “Car Evaluation” dataset from the UCI Machine Learning Repository informs the Bayesian Network. Features like buying price and maintenance cost predict the acceptability class of cars, showcasing practical Bayesian Network applications. Visualizing the Bayesian Network using the daft library provides an intuitive representation of the network structure\n\n\nConclusion:\nBayesian Networks stand out as a robust framework for modeling uncertainty and aiding informed decision-making. Their graphical representation offers an intuitive understanding of complex systems and facilitates predictions based on observed evidence. Applied across diverse domains, from healthcare to finance, Bayesian Networks remain valuable tools for probabilistic modeling and reasoning.\n\n\nDataset Source:\nThe dataset used in this blog is sourced from the UCI Machine Learning Repository -UCI Machine Learning Repository-Car_Dataset. It provides information about car features and their acceptability class.\nThe Image is taken from- https://www.turing.com/kb/an-overview-of-bayesian-networks-in-ai"
  }
]