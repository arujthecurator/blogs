{"title":"Classification: Using Balanced Bagging Classifier","markdown":{"yaml":{"title":"Classification: Using Balanced Bagging Classifier","author":"Aruj Nayak","date":"2023-12-06","categories":["Classification"],"image":"classification.jpg"},"headingText":"Classification: Using Balanced Bagging Classifier","containsRefs":false,"markdown":"\n\n# Introduction:\nIn the realm of machine learning, dealing with class imbalance is a common challenge that can significantly impact model performance. In this blog, we delve into the application of balanced bagging classifiers using the Pima Indians Diabetes dataset. Additionally, we harness the power of Principal Component Analysis (PCA) to gain visual insights into decision boundaries within a reduced-dimensional space.\n\n# Defining the Decision Boundary Method:\nWe begin by importing essential libraries and defining a function to plot the decision boundary so as to properly visualize how target variable is classified.\n\n``` {python}\n# Import the libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom imblearn.ensemble import BalancedBaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Define a function to plot decision boundaries\ndef plot_decision_boundary(model, X, y, title):\n    h = .02  # step size in the mesh\n    x_min, x_max = X.iloc[:, 0].min() - 1, X.iloc[:, 0].max() + 1\n    y_min, y_max = X.iloc[:, 1].min() - 1, X.iloc[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    plt.contourf(xx, yy, Z, alpha=0.3)\n    plt.scatter(X.iloc[:, 0], X.iloc[:, 1], c=y, edgecolors='k', marker='o', s=80, linewidth=0.8)\n    plt.title(title)\n    plt.xlabel(X.columns[0])\n    plt.ylabel(X.columns[1])\n    plt.show()\n\n```\n\n# Class Distribution Before Balancing:\nFor this blog, we will use the Pima Indians Diabetes (Pima) dataset which is freely available online. This dataset contains crucial health parameters, like BloodPressure, SkinThickness, etc. Each record in this dataset describes the medical details of a female, and the prediction is the onset of diabetes within the next five years. A visual inspection of the class distribution before any balancing techniques reveals the initial state of the data.\n\n```{python}\n# Load the Pima dataset\nurl = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv'\ncolumn_names = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']\ndata = pd.read_csv(url, names=column_names)\n\n# Plot class distribution before balancing\nplt.figure(figsize=(8, 5))\nsns.countplot(x='Outcome', data=data)\nplt.title('Class Distribution Before Balancing')\nplt.xlabel('Outcome (0: No Diabetes, 1: Diabetes)')\nplt.ylabel('Count')\nplt.show()\n```\n\n# Using Random Under-Sampling:\nTo address class imbalance, we employ the Random Under-Sampling technique, strategically removing instances from the majority class. The resulting balanced dataset is visualized for a clearer understanding.\n\n``` {python}\n# Apply balanced sampling techniques using RandomUnderSampler\nfrom imblearn.under_sampling import RandomUnderSampler\n\n# Split the data into features (X) and target variable (y)\nX = data.drop('Outcome', axis=1)\ny = data['Outcome']\n\n# Use RandomUnderSampler for under-sampling\nrus = RandomUnderSampler(random_state=42)\nX_resampled, y_resampled = rus.fit_resample(X, y)\n\n# Create a DataFrame with the balanced data\nbalanced_data = pd.DataFrame(X_resampled, columns=column_names[:-1])\nbalanced_data['Outcome'] = y_resampled\n\n# Visualize the balanced class distribution\nplt.figure(figsize=(8, 5))\nsns.countplot(x='Outcome', data=balanced_data)\nplt.title('Class Distribution After Balancing')\nplt.xlabel('Outcome (0: No Diabetes, 1: Diabetes)')\nplt.ylabel('Count')\nplt.show()\n```\n\n# Model Training with PCA:\nThis balanced data is now utilized for model training. We split the dataset, standardize features, and integrate PCA to reduce dimensionality, capturing essential patterns for our models.\n\n``` {python}\n#Assign the features\nfeature_names = column_names[:-1]\n\n# Split the data into features (X) and target variable (y)\nX = balanced_data.drop('Outcome', axis=1)\ny = balanced_data['Outcome']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Standardize the features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Apply PCA to reduce dimensionality\npca = PCA(n_components=2)\nX_train_pca = pca.fit_transform(X_train_scaled)\nX_test_pca = pca.transform(X_test_scaled)\n```\n\n# Decision Boundaries of the Base Model and Balanced Bagging Classifier:\nFinally, a base model (Decision Tree) and an ensemble model (the Balanced Bagging Classifier) are trained. Their decision boundaries are then plotted so as to visualize the difference in classification of the target variable after balancing the classes. We have plotted the decision boundaries of both the base model (trained on the imbalanced dataset) and the Balanced Bagging Classifier within the reduced-dimensional space obtained through PCA.\n\n``` {python}\n# Define and train a base model (e.g., Decision Tree)\nbase_model = DecisionTreeClassifier(random_state=42)\nbase_model.fit(X_train_pca, y_train)\n\n# Initialize and train a Balanced Bagging Classifier\nbalanced_bagging_model = BalancedBaggingClassifier(base_model, random_state=42)\nbalanced_bagging_model.fit(X_train_pca, y_train)\n\n# Visualize decision boundaries of the base model\nplt.figure(figsize=(10, 6))\nplot_decision_boundary(base_model, pd.DataFrame(X_train_pca, columns=['Principal Component 1', 'Principal Component 2']), y_train, 'Decision Boundary of base model (Imbalanced Data)')\n\n# Visualize decision boundaries of the balanced bagging classifier\nplt.figure(figsize=(10, 6))\nplot_decision_boundary(balanced_bagging_model, pd.DataFrame(X_train_pca, columns=['Principal Component 1', 'Principal Component 2']), y_train, 'Decision Boundaries of Balanced Bagging Classifier')\n\n```\n\n```{python}\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n\n# Function to evaluate a model and print metrics\ndef evaluate_model(model, X, y):\n    y_pred = model.predict(X)\n    accuracy = accuracy_score(y, y_pred)\n    precision = precision_score(y, y_pred)\n    recall = recall_score(y, y_pred)\n    f1 = f1_score(y, y_pred)\n    \n    print(f\"Accuracy: {accuracy:.4f}\")\n    print(f\"Precision: {precision:.4f}\")\n    print(f\"Recall: {recall:.4f}\")\n    print(f\"F1 Score: {f1:.4f}\")\n    \n    # Confusion matrix\n    cm = confusion_matrix(y, y_pred)\n    print(\"\\nConfusion Matrix:\")\n    print(cm)\n\n# Evaluate the base model\nprint(\"Base Model Evaluation:\")\nevaluate_model(base_model, X_test_pca, y_test)\n\n# Evaluate the improved Balanced Bagging Classifier\nprint(\"\\nImproved Balanced Bagging Classifier Evaluation:\")\nevaluate_model(balanced_bagging_model, X_test_pca, y_test)\n\n```\n\n# Conclusion:\nIn this blog, we've navigated through the intricacies of handling class imbalance using balanced bagging classifiers and illuminating decision boundaries through PCA. This approach not only improves model robustness but also enhances interpretability, crucial for making informed decisions.\n\n# Source:\nThe dataset is prepared by Jason Brownlee and can be found here- [Pima Indians Diabetes dataset ](https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv).\n\nThe Image is taken from- [https://datagalore.substack.com/p/class-imbalance-problem](https://datagalore.substack.com/p/class-imbalance-problem)\n\n","srcMarkdownNoYaml":"\n# Classification: Using Balanced Bagging Classifier\n\n# Introduction:\nIn the realm of machine learning, dealing with class imbalance is a common challenge that can significantly impact model performance. In this blog, we delve into the application of balanced bagging classifiers using the Pima Indians Diabetes dataset. Additionally, we harness the power of Principal Component Analysis (PCA) to gain visual insights into decision boundaries within a reduced-dimensional space.\n\n# Defining the Decision Boundary Method:\nWe begin by importing essential libraries and defining a function to plot the decision boundary so as to properly visualize how target variable is classified.\n\n``` {python}\n# Import the libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom imblearn.ensemble import BalancedBaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Define a function to plot decision boundaries\ndef plot_decision_boundary(model, X, y, title):\n    h = .02  # step size in the mesh\n    x_min, x_max = X.iloc[:, 0].min() - 1, X.iloc[:, 0].max() + 1\n    y_min, y_max = X.iloc[:, 1].min() - 1, X.iloc[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    plt.contourf(xx, yy, Z, alpha=0.3)\n    plt.scatter(X.iloc[:, 0], X.iloc[:, 1], c=y, edgecolors='k', marker='o', s=80, linewidth=0.8)\n    plt.title(title)\n    plt.xlabel(X.columns[0])\n    plt.ylabel(X.columns[1])\n    plt.show()\n\n```\n\n# Class Distribution Before Balancing:\nFor this blog, we will use the Pima Indians Diabetes (Pima) dataset which is freely available online. This dataset contains crucial health parameters, like BloodPressure, SkinThickness, etc. Each record in this dataset describes the medical details of a female, and the prediction is the onset of diabetes within the next five years. A visual inspection of the class distribution before any balancing techniques reveals the initial state of the data.\n\n```{python}\n# Load the Pima dataset\nurl = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv'\ncolumn_names = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']\ndata = pd.read_csv(url, names=column_names)\n\n# Plot class distribution before balancing\nplt.figure(figsize=(8, 5))\nsns.countplot(x='Outcome', data=data)\nplt.title('Class Distribution Before Balancing')\nplt.xlabel('Outcome (0: No Diabetes, 1: Diabetes)')\nplt.ylabel('Count')\nplt.show()\n```\n\n# Using Random Under-Sampling:\nTo address class imbalance, we employ the Random Under-Sampling technique, strategically removing instances from the majority class. The resulting balanced dataset is visualized for a clearer understanding.\n\n``` {python}\n# Apply balanced sampling techniques using RandomUnderSampler\nfrom imblearn.under_sampling import RandomUnderSampler\n\n# Split the data into features (X) and target variable (y)\nX = data.drop('Outcome', axis=1)\ny = data['Outcome']\n\n# Use RandomUnderSampler for under-sampling\nrus = RandomUnderSampler(random_state=42)\nX_resampled, y_resampled = rus.fit_resample(X, y)\n\n# Create a DataFrame with the balanced data\nbalanced_data = pd.DataFrame(X_resampled, columns=column_names[:-1])\nbalanced_data['Outcome'] = y_resampled\n\n# Visualize the balanced class distribution\nplt.figure(figsize=(8, 5))\nsns.countplot(x='Outcome', data=balanced_data)\nplt.title('Class Distribution After Balancing')\nplt.xlabel('Outcome (0: No Diabetes, 1: Diabetes)')\nplt.ylabel('Count')\nplt.show()\n```\n\n# Model Training with PCA:\nThis balanced data is now utilized for model training. We split the dataset, standardize features, and integrate PCA to reduce dimensionality, capturing essential patterns for our models.\n\n``` {python}\n#Assign the features\nfeature_names = column_names[:-1]\n\n# Split the data into features (X) and target variable (y)\nX = balanced_data.drop('Outcome', axis=1)\ny = balanced_data['Outcome']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Standardize the features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Apply PCA to reduce dimensionality\npca = PCA(n_components=2)\nX_train_pca = pca.fit_transform(X_train_scaled)\nX_test_pca = pca.transform(X_test_scaled)\n```\n\n# Decision Boundaries of the Base Model and Balanced Bagging Classifier:\nFinally, a base model (Decision Tree) and an ensemble model (the Balanced Bagging Classifier) are trained. Their decision boundaries are then plotted so as to visualize the difference in classification of the target variable after balancing the classes. We have plotted the decision boundaries of both the base model (trained on the imbalanced dataset) and the Balanced Bagging Classifier within the reduced-dimensional space obtained through PCA.\n\n``` {python}\n# Define and train a base model (e.g., Decision Tree)\nbase_model = DecisionTreeClassifier(random_state=42)\nbase_model.fit(X_train_pca, y_train)\n\n# Initialize and train a Balanced Bagging Classifier\nbalanced_bagging_model = BalancedBaggingClassifier(base_model, random_state=42)\nbalanced_bagging_model.fit(X_train_pca, y_train)\n\n# Visualize decision boundaries of the base model\nplt.figure(figsize=(10, 6))\nplot_decision_boundary(base_model, pd.DataFrame(X_train_pca, columns=['Principal Component 1', 'Principal Component 2']), y_train, 'Decision Boundary of base model (Imbalanced Data)')\n\n# Visualize decision boundaries of the balanced bagging classifier\nplt.figure(figsize=(10, 6))\nplot_decision_boundary(balanced_bagging_model, pd.DataFrame(X_train_pca, columns=['Principal Component 1', 'Principal Component 2']), y_train, 'Decision Boundaries of Balanced Bagging Classifier')\n\n```\n\n```{python}\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n\n# Function to evaluate a model and print metrics\ndef evaluate_model(model, X, y):\n    y_pred = model.predict(X)\n    accuracy = accuracy_score(y, y_pred)\n    precision = precision_score(y, y_pred)\n    recall = recall_score(y, y_pred)\n    f1 = f1_score(y, y_pred)\n    \n    print(f\"Accuracy: {accuracy:.4f}\")\n    print(f\"Precision: {precision:.4f}\")\n    print(f\"Recall: {recall:.4f}\")\n    print(f\"F1 Score: {f1:.4f}\")\n    \n    # Confusion matrix\n    cm = confusion_matrix(y, y_pred)\n    print(\"\\nConfusion Matrix:\")\n    print(cm)\n\n# Evaluate the base model\nprint(\"Base Model Evaluation:\")\nevaluate_model(base_model, X_test_pca, y_test)\n\n# Evaluate the improved Balanced Bagging Classifier\nprint(\"\\nImproved Balanced Bagging Classifier Evaluation:\")\nevaluate_model(balanced_bagging_model, X_test_pca, y_test)\n\n```\n\n# Conclusion:\nIn this blog, we've navigated through the intricacies of handling class imbalance using balanced bagging classifiers and illuminating decision boundaries through PCA. This approach not only improves model robustness but also enhances interpretability, crucial for making informed decisions.\n\n# Source:\nThe dataset is prepared by Jason Brownlee and can be found here- [Pima Indians Diabetes dataset ](https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv).\n\nThe Image is taken from- [https://datagalore.substack.com/p/class-imbalance-problem](https://datagalore.substack.com/p/class-imbalance-problem)\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","theme":"cosmo","title-block-banner":true,"title":"Classification: Using Balanced Bagging Classifier","author":"Aruj Nayak","date":"2023-12-06","categories":["Classification"],"image":"classification.jpg"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}