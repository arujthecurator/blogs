{"title":"Linear and Non-Linear Regression","markdown":{"yaml":{"title":"Linear and Non-Linear Regression","author":"Aruj Nayak","date":"2023-12-06","categories":["Linear & Non-Linear Regression"],"image":"regression.jpg"},"headingText":"Exploring Linear and Non-linear Regression in Machine Learning","containsRefs":false,"markdown":"\n\n\n# Introduction:\n\nLinear and non-linear regression are essential techniques in the realm of machine learning and statistics, commonly utilized for predicting numerical values based on input features. In this blog, we delve into the core concepts of linear and non-linear regression and provide practical examples using two prominent algorithms: Support Vector Machine (SVM) for linear regression and Gradient Boosting Regressor for non-linear regression.\n\n# Linear Regression:\n\nLinear regression is a straightforward and interpretable method used for modeling the relationship between a dependent variable and one or more independent variables. This method assumes a linear relationship, signifying that the change in the dependent variable is proportionate to the change in the independent variables. The equation for simple linear regression can be expressed as:\n\n\\[ y = mx + b \\]\n\nHere:\n- \\( y \\): Dependent variable\n- \\( x \\): Independent variable\n- \\( m \\): Slope of the line\n- \\( b \\): Y-intercept\n\n# Implementation:\nIn this blog, we implemented linear regression using the scikit-learn library with the Support Vector Machine (SVM) algorithm. In this context, SVM with a linear kernel functions as a linear regressor. We utilized the \"diamonds\" dataset from seaborn, considering attributes such as carat, depth, table, and dimensions (x, y, z) to predict diamond prices. The following code snippet demonstrates this-\n\n``` {python}\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\nimport seaborn as sns\n\n# Load the diamonds dataset\ndiamonds = sns.load_dataset(\"diamonds\")\n\n# Select features and target variable\nX = diamonds[['carat', 'depth', 'table', 'x', 'y', 'z']]\ny = diamonds['price']\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create and train the support vector regressor with linear kernel\nlinear_svm_model = SVR(kernel='linear')\nlinear_svm_model.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred_linear_svm = linear_svm_model.predict(X_test)\n\n# Evaluate the model\nr2_linear_svm=r2_score(y_test,y_pred_linear_svm)\nprint(f\"r2 score (Linear SVM): {r2_linear_svm}\")\n```\n\nThe actual vs predicted labels for the test data predicted by this model can be visualized in order to understand how good our model is performing-\n\n``` {python}\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Visualize predicted vs actual for linear regression\nplt.scatter(y_test, y_pred_linear_svm, alpha=0.5, label='Actual vs Predicted')\nplt.plot(np.unique(y_test), np.poly1d(np.polyfit(y_test, y_pred_linear_svm, 1))(np.unique(y_test)), color='red', label='Best Fit Line')\nplt.title(\"Linear Regression: Predicted vs Actual\")\nplt.xlabel(\"Actual Prices\")\nplt.ylabel(\"Predicted Prices\")\nplt.legend()\nplt.show()\n```\n\n# Non-Linear Regression-\n\nWhile linear regression is effective for linear relationships, real-world scenarios often involve non-linear patterns. Non-linear regression extends the concept to model relationships where the change in the dependent variable is not proportional to the change in the independent variables.\n\n# Implementation-\n\nFor non-linear regression, we employed the Gradient Boosting Regressor, an ensemble method that combines weak learners to create a robust predictive model. Using the same \"diamonds\" dataset, this algorithm can capture intricate patterns in the data. Following code show the implementation of this model-\n\n``` {python}\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.metrics import r2_score\n\n# Create and train the gradient boosting regressor\ngradient_boosting_model = GradientBoostingRegressor(n_estimators=100, random_state=42)\ngradient_boosting_model.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred_gradient_boosting = gradient_boosting_model.predict(X_test)\n\n# Evaluate the model\nr2_gradient_boosting=r2_score(y_test,y_pred_gradient_boosting)\nprint(f\"r2 score (Gradient Boosting): {r2_gradient_boosting}\")\n```\n\nAnd just like linear regression, we can visualize the predictions made by this model-\n``` {python}\n# Visualize predicted vs actual for non-linear regression\nplt.scatter(y_test, y_pred_gradient_boosting, alpha=0.5, label='Actual vs Predicted')\nplt.plot(np.unique(y_test), np.poly1d(np.polyfit(y_test, y_pred_gradient_boosting, 1))(np.unique(y_test)), color='red', label='Best Fit Line')\nplt.title(\"Non-linear Regression: Predicted vs Actual\")\nplt.xlabel(\"Actual Prices\")\nplt.ylabel(\"Predicted Prices\")\nplt.legend()\nplt.show()\n```\n\n# Conclusion:\nThus, linear and non-linear regression are versatile tools for modeling relationships in data. Linear regression provides simplicity and interpretability, making it suitable for scenarios with linear dependencies. Conversely, non-linear regression techniques, like the Gradient Boosting Regressor, offer the flexibility to capture complex patterns in the data, making them valuable in various applications.\n\nWhen working with real-world datasets, understanding the nature of relationships within the data is crucial for choosing the appropriate regression model. Linear and non-linear regression methods complement each other, enabling data scientists and analysts to address a diverse array of predictive modeling tasks with precision and accuracy.\n\n# Source:\n\nThe image is taken from- [https://www.skillshare.com/en/classes/Data-Analysis-What-is-Non-Linear-Regression/694158109](https://www.skillshare.com/en/classes/Data-Analysis-What-is-Non-Linear-Regression/694158109)","srcMarkdownNoYaml":"\n\n# Exploring Linear and Non-linear Regression in Machine Learning\n\n# Introduction:\n\nLinear and non-linear regression are essential techniques in the realm of machine learning and statistics, commonly utilized for predicting numerical values based on input features. In this blog, we delve into the core concepts of linear and non-linear regression and provide practical examples using two prominent algorithms: Support Vector Machine (SVM) for linear regression and Gradient Boosting Regressor for non-linear regression.\n\n# Linear Regression:\n\nLinear regression is a straightforward and interpretable method used for modeling the relationship between a dependent variable and one or more independent variables. This method assumes a linear relationship, signifying that the change in the dependent variable is proportionate to the change in the independent variables. The equation for simple linear regression can be expressed as:\n\n\\[ y = mx + b \\]\n\nHere:\n- \\( y \\): Dependent variable\n- \\( x \\): Independent variable\n- \\( m \\): Slope of the line\n- \\( b \\): Y-intercept\n\n# Implementation:\nIn this blog, we implemented linear regression using the scikit-learn library with the Support Vector Machine (SVM) algorithm. In this context, SVM with a linear kernel functions as a linear regressor. We utilized the \"diamonds\" dataset from seaborn, considering attributes such as carat, depth, table, and dimensions (x, y, z) to predict diamond prices. The following code snippet demonstrates this-\n\n``` {python}\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\nimport seaborn as sns\n\n# Load the diamonds dataset\ndiamonds = sns.load_dataset(\"diamonds\")\n\n# Select features and target variable\nX = diamonds[['carat', 'depth', 'table', 'x', 'y', 'z']]\ny = diamonds['price']\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create and train the support vector regressor with linear kernel\nlinear_svm_model = SVR(kernel='linear')\nlinear_svm_model.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred_linear_svm = linear_svm_model.predict(X_test)\n\n# Evaluate the model\nr2_linear_svm=r2_score(y_test,y_pred_linear_svm)\nprint(f\"r2 score (Linear SVM): {r2_linear_svm}\")\n```\n\nThe actual vs predicted labels for the test data predicted by this model can be visualized in order to understand how good our model is performing-\n\n``` {python}\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Visualize predicted vs actual for linear regression\nplt.scatter(y_test, y_pred_linear_svm, alpha=0.5, label='Actual vs Predicted')\nplt.plot(np.unique(y_test), np.poly1d(np.polyfit(y_test, y_pred_linear_svm, 1))(np.unique(y_test)), color='red', label='Best Fit Line')\nplt.title(\"Linear Regression: Predicted vs Actual\")\nplt.xlabel(\"Actual Prices\")\nplt.ylabel(\"Predicted Prices\")\nplt.legend()\nplt.show()\n```\n\n# Non-Linear Regression-\n\nWhile linear regression is effective for linear relationships, real-world scenarios often involve non-linear patterns. Non-linear regression extends the concept to model relationships where the change in the dependent variable is not proportional to the change in the independent variables.\n\n# Implementation-\n\nFor non-linear regression, we employed the Gradient Boosting Regressor, an ensemble method that combines weak learners to create a robust predictive model. Using the same \"diamonds\" dataset, this algorithm can capture intricate patterns in the data. Following code show the implementation of this model-\n\n``` {python}\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.metrics import r2_score\n\n# Create and train the gradient boosting regressor\ngradient_boosting_model = GradientBoostingRegressor(n_estimators=100, random_state=42)\ngradient_boosting_model.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred_gradient_boosting = gradient_boosting_model.predict(X_test)\n\n# Evaluate the model\nr2_gradient_boosting=r2_score(y_test,y_pred_gradient_boosting)\nprint(f\"r2 score (Gradient Boosting): {r2_gradient_boosting}\")\n```\n\nAnd just like linear regression, we can visualize the predictions made by this model-\n``` {python}\n# Visualize predicted vs actual for non-linear regression\nplt.scatter(y_test, y_pred_gradient_boosting, alpha=0.5, label='Actual vs Predicted')\nplt.plot(np.unique(y_test), np.poly1d(np.polyfit(y_test, y_pred_gradient_boosting, 1))(np.unique(y_test)), color='red', label='Best Fit Line')\nplt.title(\"Non-linear Regression: Predicted vs Actual\")\nplt.xlabel(\"Actual Prices\")\nplt.ylabel(\"Predicted Prices\")\nplt.legend()\nplt.show()\n```\n\n# Conclusion:\nThus, linear and non-linear regression are versatile tools for modeling relationships in data. Linear regression provides simplicity and interpretability, making it suitable for scenarios with linear dependencies. Conversely, non-linear regression techniques, like the Gradient Boosting Regressor, offer the flexibility to capture complex patterns in the data, making them valuable in various applications.\n\nWhen working with real-world datasets, understanding the nature of relationships within the data is crucial for choosing the appropriate regression model. Linear and non-linear regression methods complement each other, enabling data scientists and analysts to address a diverse array of predictive modeling tasks with precision and accuracy.\n\n# Source:\n\nThe image is taken from- [https://www.skillshare.com/en/classes/Data-Analysis-What-is-Non-Linear-Regression/694158109](https://www.skillshare.com/en/classes/Data-Analysis-What-is-Non-Linear-Regression/694158109)"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","theme":"cosmo","title-block-banner":true,"title":"Linear and Non-Linear Regression","author":"Aruj Nayak","date":"2023-12-06","categories":["Linear & Non-Linear Regression"],"image":"regression.jpg"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}