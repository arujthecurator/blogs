{
  "hash": "89dbda79698b6910d05eaf7d9a555fe4",
  "result": {
    "markdown": "---\ntitle: \"Clustering: Agglomerative Clustering\"\nauthor: \"Aruj Nayak\"\ndate: \"2023-12-06\"\ncategories: [Clustering]\nimage: \"clustering.jpg\"\n---\n\n# Agglomerative Clustering Demonstrated Using Seeds Dataset\n\n# Introduction:\nIn the realm of unsupervised machine learning, agglomerative clustering stands out as a powerful tool for unraveling hidden structures within datasets. In this blog post, we will see how agglomerative clustering works while exploring its application on the Seeds dataset. The Seeds dataset which is a collection of measurements related to various seed properties, is a very simple data set to understand this clustering technique.\n\n# Seeds Dataset:\nThe Seeds dataset, sourced from the UCI Machine Learning Repository, encapsulates the characteristics of three different varieties of wheat seeds: Kama, Rosa, and Canadian. The dataset comprises of seven distinct features ('V1' to 'V7'),which are based on seed atrributes like area, perimeter, compactness, etc. measured for each seed.\n\n# Exploration and Preprocessing:\nBefore delving into clustering, it's crucial to acquaint ourselves with the features at hand. Visualization aids in this process, and histograms or scatter plots can reveal the distribution and relationships within the dataset. Moreover, standardizing or scaling features may be necessary to ensure each contributes equally to the clustering process.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import datasets\n\n# Load the Seeds dataset\nseeds = datasets.fetch_openml(name='seeds', version=1)\nX = pd.DataFrame(data=seeds.data, columns=seeds.feature_names)\n\n# Visualize features (example: scatter plot of v1 and v2)\nplt.scatter(X['V1'], X['V2'])\nplt.title('Scatter Plot of Feature v1 vs. Feature v2')\nplt.xlabel('Feature v1')\nplt.ylabel('Feature v2')\nplt.show()\n\n# Standardize features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nC:\\Users\\USER\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\datasets\\_openml.py:1022: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n  warn(\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-2-output-2.png){width=585 height=449}\n:::\n:::\n\n\n# Understanding Agglomerative Clustering:\nAgglomerative clustering builds clusters hierarchically by successively merging or agglomerating individual data points or clusters. The process continues until all data points belong to a single cluster or a specified number of clusters is reached. The choice of linkage method, such as 'ward,' 'complete,' or 'average,' influences how the distance between clusters is calculated during the agglomeration process.\n\n# Clustering Seeds with Ward Linkage:\nThe Ward linkage minimizes the variance within clusters, aiming to create compact and spherical clusters. This method is sensitive to the distribution of data points and often results in well-defined clusters.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.metrics import silhouette_score\n\n# Perform agglomerative clustering with Ward linkage\nn_clusters = 3\nagg_clustering_ward = AgglomerativeClustering(n_clusters=n_clusters, linkage='ward')\nlabels_ward = agg_clustering_ward.fit_predict(X_scaled)\n\n# Silhouette score for Ward linkage\nsilhouette_avg_ward = silhouette_score(X_scaled, labels_ward)\nprint(f'Ward Linkage - Silhouette Score: {silhouette_avg_ward}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nWard Linkage - Silhouette Score: 0.3926339709101015\n```\n:::\n:::\n\n\n# Clustering Seeds with Complete Linkage:\nIn contrast, the Complete linkage measures the maximum distance between clusters. It tends to form clusters with similar shapes and sizes, making it less sensitive to outliers compared to Ward linkage.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n# Perform agglomerative clustering with Complete linkage\nagg_clustering_complete = AgglomerativeClustering(n_clusters=n_clusters, linkage='complete')\nlabels_complete = agg_clustering_complete.fit_predict(X_scaled)\n\n# Silhouette score for Complete linkage\nsilhouette_avg_complete = silhouette_score(X_scaled, labels_complete)\nprint(f'Complete Linkage - Silhouette Score: {silhouette_avg_complete}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nComplete Linkage - Silhouette Score: 0.35019845816108097\n```\n:::\n:::\n\n\n# Visual Comparison\nLet's visualize the clusters formed by both Ward and Complete linkage methods to observe the distinctions. The scatter plots showcase how each method influences the grouping of seeds based on their features.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n# Visualize clusters for Ward Linkage\nplt.scatter(X['V1'], X['V2'], c=labels_ward, cmap='viridis')\nplt.title('Agglomerative Clustering (Ward Linkage) of Seeds Dataset')\nplt.xlabel('Feature v1')\nplt.ylabel('Feature v2')\nplt.show()\n\n# Visualize clusters for Complete Linkage\nplt.scatter(X['V1'], X['V2'], c=labels_complete, cmap='viridis')\nplt.title('Agglomerative Clustering (Complete Linkage) of Seeds Dataset')\nplt.xlabel('Feature v1')\nplt.ylabel('Feature v2')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-1.png){width=585 height=449}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-2.png){width=585 height=449}\n:::\n:::\n\n\n# Cluster Profiles\nAs clusters materialize, it's time to interpret their meaning. Examining the average values of each feature within a cluster unveils distinct profiles. Are certain clusters dominated by larger seeds, or do others exhibit unique combinations of area and compactness?\n\n# Validation and Interpretation Using Silhouette Score:\nThe silhouette score is a metric used to calculate the goodness of a clustering technique. It measures how similar an object is to its own cluster (cohesion) compared to other clusters (separation). The silhouette score ranges from -1 to 1, where a higher score indicates better-defined clusters. A score around 0 indicates overlapping clusters. Validation metrics, such as silhouette scores, offer quantitative measures of clustering quality. Moreover, comparing clustering results across different linkage methods can shed light on the sensitivity of the algorithm to choices made during clustering.\n\n# Conclusion\nIn conclusion, agglomerative clustering's hierarchical nature provides a nuanced understanding of seed properties, enabling us to discern patterns that might otherwise remain concealed. This blog tried to illustrate the basic usage of agglomerative clustering to understand how unsupervised machine learning algorithms can be used.\n\n# Source:\nImage is taken from- [https://online.keele.ac.uk/clustering-and-cluster-analysis-an-explainer/](https://online.keele.ac.uk/clustering-and-cluster-analysis-an-explainer/)\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}