{
  "hash": "9076996f6eaca03775d6a2b959133b54",
  "result": {
    "markdown": "---\ntitle: \"Classification: Using Balanced Bagging Classifier\"\nauthor: \"Aruj Nayak\"\ndate: \"2023-12-06\"\ncategories: [Classification]\nimage: \"classification.jpg\"\n---\n\n# Classification: Using Balanced Bagging Classifier\n\n# Introduction:\nIn the realm of machine learning, dealing with class imbalance is a common challenge that can significantly impact model performance. In this blog, we delve into the application of balanced bagging classifiers using the Pima Indians Diabetes dataset. Additionally, we harness the power of Principal Component Analysis (PCA) to gain visual insights into decision boundaries within a reduced-dimensional space.\n\n# Defining the Decision Boundary Method:\nWe begin by importing essential libraries and defining a function to plot the decision boundary so as to properly visualize how target variable is classified.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n# Import the libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom imblearn.ensemble import BalancedBaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Define a function to plot decision boundaries\ndef plot_decision_boundary(model, X, y, title):\n    h = .02  # step size in the mesh\n    x_min, x_max = X.iloc[:, 0].min() - 1, X.iloc[:, 0].max() + 1\n    y_min, y_max = X.iloc[:, 1].min() - 1, X.iloc[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    plt.contourf(xx, yy, Z, alpha=0.3)\n    plt.scatter(X.iloc[:, 0], X.iloc[:, 1], c=y, edgecolors='k', marker='o', s=80, linewidth=0.8)\n    plt.title(title)\n    plt.xlabel(X.columns[0])\n    plt.ylabel(X.columns[1])\n    plt.show()\n```\n:::\n\n\n# Class Distribution Before Balancing:\nFor this blog, we will use the Pima Indians Diabetes (Pima) dataset which is freely available online. This dataset contains crucial health parameters, like BloodPressure, SkinThickness, etc. Each record in this dataset describes the medical details of a female, and the prediction is the onset of diabetes within the next five years. A visual inspection of the class distribution before any balancing techniques reveals the initial state of the data.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# Load the Pima dataset\nurl = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv'\ncolumn_names = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']\ndata = pd.read_csv(url, names=column_names)\n\n# Plot class distribution before balancing\nplt.figure(figsize=(8, 5))\nsns.countplot(x='Outcome', data=data)\nplt.title('Class Distribution Before Balancing')\nplt.xlabel('Outcome (0: No Diabetes, 1: Diabetes)')\nplt.ylabel('Count')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-1.png){width=667 height=449}\n:::\n:::\n\n\n# Using Random Under-Sampling:\nTo address class imbalance, we employ the Random Under-Sampling technique, strategically removing instances from the majority class. The resulting balanced dataset is visualized for a clearer understanding.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n# Apply balanced sampling techniques using RandomUnderSampler\nfrom imblearn.under_sampling import RandomUnderSampler\n\n# Split the data into features (X) and target variable (y)\nX = data.drop('Outcome', axis=1)\ny = data['Outcome']\n\n# Use RandomUnderSampler for under-sampling\nrus = RandomUnderSampler(random_state=42)\nX_resampled, y_resampled = rus.fit_resample(X, y)\n\n# Create a DataFrame with the balanced data\nbalanced_data = pd.DataFrame(X_resampled, columns=column_names[:-1])\nbalanced_data['Outcome'] = y_resampled\n\n# Visualize the balanced class distribution\nplt.figure(figsize=(8, 5))\nsns.countplot(x='Outcome', data=balanced_data)\nplt.title('Class Distribution After Balancing')\nplt.xlabel('Outcome (0: No Diabetes, 1: Diabetes)')\nplt.ylabel('Count')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-1.png){width=668 height=449}\n:::\n:::\n\n\n# Model Training with PCA:\nThis balanced data is now utilized for model training. We split the dataset, standardize features, and integrate PCA to reduce dimensionality, capturing essential patterns for our models.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n#Assign the features\nfeature_names = column_names[:-1]\n\n# Split the data into features (X) and target variable (y)\nX = balanced_data.drop('Outcome', axis=1)\ny = balanced_data['Outcome']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Standardize the features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Apply PCA to reduce dimensionality\npca = PCA(n_components=2)\nX_train_pca = pca.fit_transform(X_train_scaled)\nX_test_pca = pca.transform(X_test_scaled)\n```\n:::\n\n\n# Decision Boundaries of the Base Model and Balanced Bagging Classifier:\nFinally, a base model (Decision Tree) and an ensemble model (the Balanced Bagging Classifier) are trained. Their decision boundaries are then plotted so as to visualize the difference in classification of the target variable after balancing the classes. We have plotted the decision boundaries of both the base model (trained on the imbalanced dataset) and the Balanced Bagging Classifier within the reduced-dimensional space obtained through PCA.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n# Define and train a base model (e.g., Decision Tree)\nbase_model = DecisionTreeClassifier(random_state=42)\nbase_model.fit(X_train_pca, y_train)\n\n# Initialize and train a Balanced Bagging Classifier\nbalanced_bagging_model = BalancedBaggingClassifier(base_model, random_state=42)\nbalanced_bagging_model.fit(X_train_pca, y_train)\n\n# Visualize decision boundaries of the base model\nplt.figure(figsize=(10, 6))\nplot_decision_boundary(base_model, pd.DataFrame(X_train_pca, columns=['Principal Component 1', 'Principal Component 2']), y_train, 'Decision Boundary of base model (Imbalanced Data)')\n\n# Visualize decision boundaries of the balanced bagging classifier\nplt.figure(figsize=(10, 6))\nplot_decision_boundary(balanced_bagging_model, pd.DataFrame(X_train_pca, columns=['Principal Component 1', 'Principal Component 2']), y_train, 'Decision Boundaries of Balanced Bagging Classifier')\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-6-output-1.png){width=810 height=523}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-6-output-2.png){width=810 height=523}\n:::\n:::\n\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n\n# Function to evaluate a model and print metrics\ndef evaluate_model(model, X, y):\n    y_pred = model.predict(X)\n    accuracy = accuracy_score(y, y_pred)\n    precision = precision_score(y, y_pred)\n    recall = recall_score(y, y_pred)\n    f1 = f1_score(y, y_pred)\n    \n    print(f\"Accuracy: {accuracy:.4f}\")\n    print(f\"Precision: {precision:.4f}\")\n    print(f\"Recall: {recall:.4f}\")\n    print(f\"F1 Score: {f1:.4f}\")\n    \n    # Confusion matrix\n    cm = confusion_matrix(y, y_pred)\n    print(\"\\nConfusion Matrix:\")\n    print(cm)\n\n# Evaluate the base model\nprint(\"Base Model Evaluation:\")\nevaluate_model(base_model, X_test_pca, y_test)\n\n# Evaluate the improved Balanced Bagging Classifier\nprint(\"\\nImproved Balanced Bagging Classifier Evaluation:\")\nevaluate_model(balanced_bagging_model, X_test_pca, y_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBase Model Evaluation:\nAccuracy: 0.5926\nPrecision: 0.5424\nRecall: 0.6531\nF1 Score: 0.5926\n\nConfusion Matrix:\n[[32 27]\n [17 32]]\n\nImproved Balanced Bagging Classifier Evaluation:\nAccuracy: 0.6667\nPrecision: 0.6226\nRecall: 0.6735\nF1 Score: 0.6471\n\nConfusion Matrix:\n[[39 20]\n [16 33]]\n```\n:::\n:::\n\n\n# Conclusion:\nIn this blog, we've navigated through the intricacies of handling class imbalance using balanced bagging classifiers and illuminating decision boundaries through PCA. This approach not only improves model robustness but also enhances interpretability, crucial for making informed decisions.\n\n# Source:\nThe dataset is prepared by Jason Brownlee and can be found here- [Pima Indians Diabetes dataset ](https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv).\n\nThe Image is taken from- [https://datagalore.substack.com/p/class-imbalance-problem](https://datagalore.substack.com/p/class-imbalance-problem)\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}